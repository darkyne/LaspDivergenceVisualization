TODO:

-Fake app qui peut run sur les nodes pour générer des valeurs et utiliser un CRDT.
-Script permettant de simuler des partitions.
-Implémentation du ORSWOT sur base du ORSET. FINALEMENT DEJA FOURNIS!
==================================================================================================================================
==================================================================================================================================
FAKE APP:

Remarque: le node sur lequel on join aura tendance à converger plus vite que les autres.

J'ai une fonction 			
launchExperiment(ExperimentNumber, NodeToJoin, CRDT_Id, TotalNumberOfNodes, SendingSpeed, NumberOfValues, GeneratingUnderPartition)

Voici sa spec:

%Launch an experiment for the current node
% IN:
%Specify a number for the experiment (used to write the result file)
%Specify the node to join to create the cluster
%Specify a CRDT_ID (format as <<"setX">>)
%Specify the Total Number of Nodes taking part of the experiment
%Specify the Sending Speed (as the number of ms between each send, considering one node), 0 means the fastest possible
%Specify the Number of Values each node will have to generate and send on the CRDT
%Specify (with a boolean) if you want the node to join the cluster then generate and send values. Or rather generate and send values on the isolated CRDT (as if it was under partition) then join.
% OUT:
%The node will generate the number of values and send them, trying to achieve the specified Sending Speed.
%It will join the cluster before or after sending the values based on GeneratingUnderPartition
%The time required to generate and send the values, the time required after the generation to reach convergence and all the paramters are written to a file
%The file name will be in the folder /lasp/Memoire/Mesures with the name Exp+ExperimentNumber+_Node+TheActualNodeId

Elle fonctionne pour l'instant.
Petits défauts:
La vitesse à laquelle les datas sont envoyés sont influencés par la vitesse donnée en argument mais n'envoit pas précisément à cette vitesse car il y a des ralentissements dûs à la machine.
Cela n'influence pas les mesures obtenues.

INFO IMPORTANTE:
La réelle mesure du temps pour converger (cas partition false) = EllapsedTime + RequiredTimeToSend - MaximumRequiredTimeToSend(biggest from all nodes).
Comme ça, on a bien le temps entre le moment où tous les nodes ont envoyés leurs données et le moment où le node a bien récupéré toutes les données du cluster.
Cela nécessite donc d'analyser l'ensemble des fichiers d'une expérience (regarder chaque node) pour pouvoir finalement calculer les convergence time.

REMARQUES: 
Quand le cluster comporte beaucoup de nodes, la disparité entre les nodes grandit énormément. Par exemple avec un cluster de 20 nodes, où chaque node génère 100values et on attend que le node récupère les 2.000 values:
Ils vont convergés par vague. Par exemple au bout de 10 secondes, 10 nodes vont converger. Puis il faudra par exemple 10 seconde de plus pour que les 10 nodes suviantes convergent d'un coup.
En gros je pense qu'il y a un petit broadcast (par exemple aux voisins proches) quand un node converge, ce qui fait qu'ils convergent par vague. Pour un petit nombre de nodes, ils convergent tous en même temps du coup.

10 nodes locaux: ils convergent tous d'un coup. Et ce, que je join avant ou après de send les values.

TODO pour dimanche: Ajouter des experiences avec des removes.
Faire en sorte que ça ne fasse que du aw_set. Il n'y a pas besoin de considérer aussi les orset, je pense.
==================================================================================================================================
==================================================================================================================================
Injection de partition:
Je join avant de générer et envoyer les values pour faire qu'il n'y ai pas de partition.
Je peux aussi générer et envoyer les values puis join, ce qui revient à être sous partition pendant l'envoi des données.
Le fait de jouer sur le fait de join ou quit le cluster permet, je pense, de simuler une partition.



WHAT I HAVE:
==================================================================================================================================
==================================================================================================================================
launchExperimentAdding:

launchExperimentAdding(ExperimentNumber, NodeToJoin, CRDT_Id, TotalNumberOfNodes, SendingSpeed, NumberOfValues, GeneratingUnderPartition)

On commence avec un aw_set vide.
Tous les nodes vont ajouter des values dedans.
On détecte lorsqu'un node a détecté l'ensemble des values.
Ex: On a 10 nodes, chaque node produit 10 values, quand un node détecte 100 values, il a convergé.
Mesure qu'on peut calculer et qui m'a l'air intéressante:
le temps entre le moment où le node le plus lent a fini d'add ses 10 values et le moment où on détecte les 100 values.
Avec les paramètres on peut jouer sur plusieurs facteurs.

TODO: Permettre de faire un add_all ou des add normaux. En gros est-ce qu'on peut tout add d'un coup ou progressivement avec une certaine vitesse ?
==================================================================================================================================
==================================================================================================================================
launchingExperimentRemoving:


On commence avec un aw_set déjà rempli avec un nombre connu de values, ex 100values.
Comment faire en sorte que tous les nodes commencent avec ce même CRDT déjà rempli?
Idée1: 	Ils commencent tous par add les values. Ensuite on attend un certain temps ex:10secondes
		Puis ils commencent à retirer les values. Exemple il y a 100 values 1,2,3,4,...100. Et chaque node va retire 10 values (ex node1 retire 1,2,3...10).
		Et on détecte quand on a 0 values dans le set.
		Ca peut être bien mais: 
		-Il faut être sûr qu'ils aient tous convergés avant de commencer à retirer des valeurs. Sinon on pourrait avoir des adds et des removes et ça foutrait le bordel.
		Régler ça: 	Attendre suffisemment de temps après avoir add les valeurs pour être sûr qu'ils ont tous convergé. Mais c'est pas ouf.
					Add toutes les valeurs d'un coup. A voir si c'est possible d'add un grand nombre de valeurs d'un coup en un seul add. Pour ça: add_all plutôt que add
					Le mieux: Hardcoder le set dès le début. Style: {state_awset, {[{"a", [{a, 2}]}],{[{a, 2}], []}}},
				
		-Il faut savoir quand le node le plus lent a finit de retirer ses 10 valeurs. C'est le temps entre ce moment et le moment où on détecte 0 valeurs qui est intéressant.
		
pareil que pour launchExperimentAdding permettre de retirer un par un ou tout d'un coup.



PROBLEME ACTUEL:


Comment faire pour lancer des experiences les unes derrières les autres?
Car je dois attendre que l'expérience soit terminée sur TOUS les nodes pour pouvoir lancer l'expérience suivante.
Afin d'éviter des décallages.
Comment envoyer un signal style à tel moment vous démarez.
Se baser sur erlang:system_time ?
